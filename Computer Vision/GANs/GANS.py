
# Importing the libraries
from __future__ import print_function
import torch
import torch.nn as nn
from torch.nn.modules.activation import LeakyReLU, Sigmoid
from torch.nn.modules.batchnorm import BatchNorm2d
import torch.nn.parallel
import torch.optim as optim
import torch.utils.data
from torch.utils.data import dataloader
import torchvision.datasets as dset
import torchvision.transforms as transforms
import torchvision.utils as vutils
from torch.autograd import Variable


torch.cuda.is_available()
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)
batch_size = 64
image_size = 64


def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        m.weight.data.normal_(0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        m.weight.data.normal_(1.0, 0.02)
        m.bias.data.fill_(0)


# Generator
class Generator(nn.Module):

    def __init__(self) -> None:
        super(Generator, self).__init__()
        #  WE CREATE META MODULE, Will involve all architecture of NN
        self.main = nn.Sequential(
            # inmverse convolution (GENAREte Fake images) (size of input/size of vector,number of features maps of output,size of kernel (size of 4byu 4),stride,padding)
            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias=False),
            nn.BatchNorm2d(512),  # normalise feature maps
            nn.ReLU(True),  # used to break Linearity
            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(True),
            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(True),
            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(True),
            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),
            # used to break linearity and get it between -1 and 1 and be centered around zero.
            nn.Tanh()
        )

    def forward(self, input):
        output = self.main(input)  # forward propagate signal through NN
        return output


class Discriminator(nn.Module):

    def __init__(self) -> None:
        super(Discriminator, self).__init__()
        self.main = nn.Sequential(
            nn.Conv2d(3, 64, 4, 2, 1, bias=False),  # normal convolution
            nn.LeakyReLU(0.2, inplace=True),  # angle of negative slope
            nn.Conv2d(64, 128, 4, 2, 1, bias=False),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(128, 256, 4, 2, 1, bias=False),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(256, 512, 4, 2, 1, bias=False),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(512, 1, 4, 1, 0, bias=False),
            nn.Sigmoid()  # we use sigmoid to get values between 0,1
        )

    def forward(self, input):
        output = self.main(input)
        # we will flatten the results of the convolutions into a SINGLE 1d FLATTEN VECTOR
        return output.view(-1)


def main():
    print("STARTING")
    # transform
    transform = transforms.Compose([transforms.Resize(image_size), transforms.ToTensor(
    ), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

    # load Dataset
    dataset = dset.CIFAR10(root="./data", download=True, transform=transform)
    dataloader = torch.utils.data.DataLoader(
        dataset, batch_size, shuffle=True, num_workers=4)

    generator_1 = Generator()  # create genate NN
    generator_1.apply(weights_init)  # apply weights to NN modules

    # discriminatgor
    discriminator = Discriminator()
    discriminator.apply(weights_init)

    # training NNs
    criterion = nn.BCELoss()  # Binary cross entropy error loss
    optimizer_Discriminator = optim.Adam(
        discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))

    optimizer_Generator = optim.Adam(
        generator_1.parameters(), lr=0.0002, betas=(0.5, 0.999))

    print("STARTING EPOCHS")

    for epoch in range(25):  # 25 epochs

        for i, data in enumerate(dataloader, 0):

            """
            1 Step Update Weights of Discriminator
            """
            discriminator.zero_grad()  # initialise to 0 to the gradients of the discriminator

            real, labels = data  # we get the real images
            input = Variable(real)
            # will return size of minibatch
            target = Variable(torch.ones(input.size()[0]))
            output = discriminator(input)
            error_discriminator_real = criterion(output, target)

            # Training the discriminator with a fake image generated by the generator
            noise = Variable(torch.randn(input.size()[0], 100, 1, 1))
            fake = generator_1(noise)
            target = Variable(torch.zeros(input.size()[0]))
            output = discriminator(fake.detach())
            error_discriminator_fake = criterion(output, target)

            # Backpropagating the total error
            error_discriminator = error_discriminator_real + error_discriminator_fake
            error_discriminator.backward()
            optimizer_Discriminator.step()

            """
            2nd Step: Updating the weights of the neural network of the generator
            """

            generator_1.zero_grad()
            target = Variable(torch.ones(input.size()[0]))
            output = discriminator(fake)
            error_generator = criterion(output, target)
            error_generator.backward()
            optimizer_Generator.step()

        # We print les losses of the discriminator (Loss_D) and the generator (Loss_G).
            print('[%d/%d][%d/%d] Loss_Discriminator: %.4f Loss_Generator: %.4f' %
                  (epoch, 25, i, len(dataloader), error_discriminator.data, error_generator.data))
            if i % 100 == 0:  # Every 100 steps:
                # We save the real images of the minibatch.
                vutils.save_image(real, '%s/real_samples.png' %
                                  "./results", normalize=True)
                fake = generator_1(noise)  # We get our fake generated images.
            # We also save the fake generated images of the minibatch.
                vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' %
                                  ("./results", epoch), normalize=True)


if __name__ == "__main__":
    main()
